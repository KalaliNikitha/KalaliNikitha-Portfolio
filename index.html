<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="description" content="Portfolio of Nikitha Kalali" />
    <meta name="keywords" content="Data Engineer, Portfolio, Data Analysis, Data Visualization" />
    <title>Nikitha Kalali's Portfolio</title>
    <link rel="stylesheet" href="assets/css/main.css" />
    <style>
        body {
            background-color: #121212;
            color: white;
            font-family: 'Times New Roman', Times, serif;
            margin: 0;
            padding: 0;
        }

        header {
            background-color: #1f1f1f;
            padding: 20px;
            text-align: center;
        }

        h1 {
            color: #f0f5f390;
            margin: 0;
        }

        section {
            padding: 20px;
            margin: 20px;
            background-color: #1f1f1f;
            border-radius: 5px;
            box-shadow: 0 2px 10px rgba(255, 255, 255, 0.1);
        }

        h2 {
            color: #f0f5f390;
            padding-bottom: 5px;
        }

        .about-text {
            color: #ccc;
            max-width: 600px;
            text-align: justify;
        }
    </style>
</head>

<body>
    <header>
        <h1>Nikitha Kalali</h1>
        <p>Data Engineer</p>
    </header>

    <section id="about">
        <h2>About Me</h2>
        <div class="about-container">
            <img src="imageme.jpg" alt="Nikitha Kalali" class="profile-image">
            <div class="about-text">
                <p>Hi, I am Nikitha Kalali. My path in the data world began at Sigma Software, where I first immersed myself in the fundamentals of data management. There, I built ETL processes from the ground up, working with Apache Kafka to enable real-time data streaming. I learned to transform messy, unstructured data into clean, usable formats – a skill that would prove invaluable throughout my career.</p>
                <p>As my expertise grew, so did the complexity of my projects. I found myself designing database schemas across multiple platforms and implementing data architecture solutions that enhanced information flow across systems. This early experience taught me how to think holistically about data ecosystems.</p>
                <p>My journey continued at Amazon, where I stepped into the world of financial data analytics. Here, I tackled massive datasets, optimizing SQL queries and building automated quality checks. I enjoyed creating interactive dashboards that brought data to life for business users. The fast-paced environment pushed me to develop more efficient solutions, leading to my promotion to Senior Associate.</p>
                <p>In my elevated role, I partnered with HR and engineering teams to optimize data pipelines and ensure reliable payroll processing. I engineered scalable solutions using AWS cloud technologies, learning to balance performance with data quality. These experiences deepened my understanding of how data directly impacts business operations.</p>
                <p>Throughout this journey, I've evolved from someone who simply processed data to a professional who architects solutions that drive business value. My experience spans the full data lifecycle – from collection and storage to transformation and visualization – always focused on uncovering the stories hidden within the numbers.</p>
            </div>
        </div>
    </section>

    <section id="skills">
        <h2>Skills</h2>
        <div class="about-text">
            <h3 class="white-heading">Technical Skills</h3>
            <ul class="skills-list">
                <li><strong>Programming Languages:</strong> Python, Java, R, SAS, MATLAB, HTML/CSS, JavaScript, C++.</li>
                <li><strong>Big Data Technologies:</strong> Apache Spark, PySpark, Databricks.</li>
                <li><strong>Database/Data Technologies:</strong> SQL, T-SQL, PostgreSQL, MySQL, Oracle, MongoDB.</li>
                <li><strong>Machine Learning & AI Frameworks:</strong> TensorFlow, Keras, PyTorch.</li>
                <li><strong>Cloud Platforms:</strong> AWS, Microsoft Azure, GCP.</li>
                <li><strong>Version Control Systems:</strong> Git, Jenkins.</li>
                <li><strong>Data Visualization Tools:</strong> Tableau, PowerBI, Looker, Excel, DAX, MicroStrategy.</li>
                <li><strong>Data Warehousing:</strong> Snowflake, AWS Redshift, Data Lakes.</li>
                <li><strong>ETL and Workflow Tools:</strong> AWS Glue, Alteryx, SSIS, Azure Data Factory, Informatica-PowerCenter, Talend.</li>
            </ul>
        </div>
    </section>

    <section id="techniques">
        <h2>Techniques Used in Project</h2>
        <ul>
            <li><strong>Predictive Modeling:</strong> XGBoost, Random Forest, Gradient Boosting, LightGBM</li>
            <li><strong>Time Series Analysis:</strong> ARIMA, SARIMA, Exponential Smoothing</li>
            <li><strong>Classification Models:</strong> Logistic Regression, Decision Tree, K-Nearest Neighbors</li>
            <li><strong>Feature Engineering:</strong> PCA, One-Hot Encoding, Feature Scaling</li>
            <li><strong>Statistical Testing:</strong> ANOVA, T-tests, Chi-Square Tests</li>
            <li><strong>Optimization Techniques:</strong> Hyperparameter Tuning, Grid Search</li>
        </ul>
    </section>

   <section id="eda">
    <h2>Exploratory Data Analysis (EDA)</h2>
    <p class="about-text">Exploratory Data Analysis (EDA) plays a seminal role in examining data and serving as the very foundation for further modelling efforts. The purpose of EDA is to spot patterns, trends, and relationships among the data. Afterwards, the identified patterns, trends, and relationships aid the researcher to develop effective analytical strategies. The exploration in this initial step helps to understand the relationship between different dimensions of the dataset and make decisions regarding modeling later.</p>
    <p><a href="https://github.com/KalaliNikitha/KalaliNikitha-Portfolio.git" target="_blank">View EDA</a></p>
</section>

<!-- Methodology Section -->
<section id="methodology">
    <h2>Methodology</h2>
    <p class="about-text">The aim is to describe the forecasting methods used in our study. All the models are trained and validated by an 80/20 split.</p>
    <h3>Demand Forecasting with SARIMA:</h3>
    <p>The SARIMA model was applied to capture seasonal patterns in demand, supporting inventory management by predicting spikes and dips in sales.</p>

    <h3>Sales Forecasting Models:</h3>
    <p>Regression Models: Linear Regression, Random Forest, Gradient Boosting, K-Neighbors, XGBoost, and LightGBM provided a range of approaches, each optimized through hyperparameter tuning to forecast sales trends accurately.</p>

    <h3>Late Delivery Prediction:</h3>
    <p>Classification models (Logistic Regression, Decision Tree, Random Forest, etc.) were evaluated for delivery predictions, with Random Forest and Logistic Regression showing high accuracy in distinguishing outcomes.</p>

    <h3>Fraud Detection:</h3>
    <p>Models like Random Forest and XGBoost achieved near-perfect accuracy in fraud detection, with metrics such as ROC-AUC confirming high reliability.</p>

    <h3>Customer Segmentation with RFM Analysis:</h3>
    <p>RFM (Recency, Frequency, Monetary) analysis categorized customers by purchasing behavior, enabling targeted engagement for different loyalty segments.</p>

    <p><a href="https://github.com/KalaliNikitha/KalaliNikitha-Portfolio.git" target="_blank">View Methodology</a></p>
</section>

<!-- Final Results Section -->
<section id="results">
    <h2>Final Results</h2>
    <p class="about-text">Sales Forecasting Models and Evaluation Metrics:</p>
    <h3>SARIMA Model:</h3>
    <p>Used for short-term demand forecasting, capturing seasonal trends but limited in handling volatility.</p>

    <h3>Regression Models:</h3>
    <p>Linear Regression, Gradient Boosting, Lasso, Ridge, ElasticNet, and AdaBoost were used, with Linear Regression showing the highest R² score, though potentially overfitting.</p>

    <h3>Safety Stock and Reorder Threshold:</h3>
    <p>Calculated to prevent stockouts, optimizing inventory by adjusting stock levels based on sales forecasts.</p>

    <h3>Late Delivery Prediction:</h3>
    <p>Classification Models: Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors, and AdaBoost were evaluated; Logistic Regression showed the best accuracy, with Random Forest and AdaBoost also performing well.</p>

    <h3>Fraud Detection Models:</h3>
    <p>Models Used: Logistic Regression, Random Forest, Gradient Boosting, K-Nearest Neighbors, Decision Tree, and XGBoost all achieved high accuracy, with Random Forest showing near-perfect precision.</p>

    <h3>Evaluation:</h3>
    <p>Confusion Matrix and Precision-Recall Curves helped assess model sensitivity and specificity in fraud detection.</p>

    <h3>RFM Analysis and Customer Segmentation:</h3>
    <p>Customer Segmentation: RFM analysis segmented customers by recency, frequency, and monetary value into groups like "Loyal Customers" and "Need Attention" to inform targeted engagement.</p>

    <p><a href="https://github.com/KalaliNikitha/KalaliNikitha-Portfolio.git" target="_blank">View Results</a></p>
</section>

    <footer>
        <p>&copy; 2024 Nikitha Kalali. All rights reserved.</p>
    </footer>
</body>

</html>
